\section{State of art}\label{sec:stateofart}

\subsection{Signal-to-Noise Ratio (SNR) Optimization and Resource Management in Mixed-Signal Embedded EEG}

The acquisition of clinical-grade electroencephalographic (EEG) signals within embedded, portable form factors is fundamentally constrained by the physics of biopotential measurement and the stringent demands of edge computing \cite{lyu2026deep}. Neural signals propagating to the scalp surface are inherently weak, exhibiting amplitudes ranging from 1 to 100 µV, and occupy a highly susceptible low-frequency band typically between 0.5 Hz and 100 Hz \cite{singh2023survey}. Extracting these delicate potentials requires overcoming extreme signal attenuation across the variable conductive layers of the skull and scalp, high electrode-skin impedance interfaces \cite{gkintoni2025mapping}, and pervasive environmental interference such as 50/60 Hz power-line noise and ambient electromagnetic fields \cite{zhang2025recent}.

When high-gain, high-impedance analog sensing circuits are integrated into compact physical proximity with high-speed digital processors, the risk of signal corruption via capacitive coupling, trace crosstalk, and ground-loop noise rises exponentially \cite{porto2025bridging}. Furthermore, the operational demand of executing continuous, high-resolution data logging alongside complex algorithmic denoising on resource-constrained embedded devices rapidly induces input/output bottlenecks, memory saturation, and thermal throttling \cite{nguyen2025edge}. These systemic processing delays introduce variable acquisition latency and temporal jitter, directly threatening the precise synchronization of biomarkers which is an absolute prerequisite for maintaining the structural continuity of the EEG stream in multi-modal analysis \cite{iwama2023two}. To address these interconnected biophysical and computational challenges, the scientific community has pursued four distinct philosophical approaches over the past four years to optimize embedded architectures.

\subsubsection{Hardware-Level Isolation and Active Analog Front-End Architecture}

The foundational philosophy for optimizing the Signal-to-Noise Ratio (SNR) at the edge posits that interference must be aggressively rejected at the physical and electrical interface long before digitization occurs, thereby preventing downstream software bottlenecks \cite{xu2025analog}. Central to modern embedded Brain-Computer Interface (BCI) systems is the utilization of highly specialized, application-specific integrated circuits designed explicitly for biopotential measurement \cite{han2022wide}. These dedicated analog front-ends feature ultra-low input-referred noise floors and 24-bit delta-sigma analog-to-digital converters that provide expansive dynamic ranges \cite{chen2022124}. This wide dynamic range is highly critical for preventing amplifier saturation when baseline microvolt neural signals are superimposed with massive, millivolt-level motion artifacts during mobile use \cite{pochet2022pseudo}.

\begin{table}[H]
    \renewcommand{\arraystretch}{1.2}
    \renewcommand{\tabularxcolumn}[1]{m{#1}}
    \caption{Acquisition devices used for BCI. The table provides an overview of the different hardware devices, their specifications, and communication protocols.}
    \label{table:bci_hardware}
    \centering
    \tiny
    \begin{tabularx}{\textwidth}{@{}
        >{\raggedright\arraybackslash}X
        >{\raggedright\arraybackslash}m{1.6cm}
        >{\raggedright\arraybackslash}m{1.8cm}
        c
        c
        c
        >{\raggedright\arraybackslash}m{1.8cm}
        c @{}}
        \toprule
        \textbf{Device}                              & \textbf{Company}    & \textbf{Electrodes}  & \textbf{Channels} & \textbf{Sampling Rate} & \textbf{AFE} & \textbf{Connectivity} & \textbf{Battery} \\ \midrule
        Cyton + Daisy \cite{OpenBCI_CytonDaisy}      & OpenBCI             & Flexible / Wet / Dry & 16                & 250 Hz -- 16 kHz       & ADS1299      & RF / BLE / Wi-Fi      & 8 h              \\
        actiCAP \cite{BrainProducts_ActiCap}         & Brain Products GmbH & Flexible / Wet / Dry & 16                & 256 Hz -- 16 kHz       & --           & USB                   & 16 h             \\
        EPOC X \cite{Emotiv_EPOCX}                   & Emotiv              & Rigid / Wet          & 14                & 128 Hz                 & --           & BLE / Bluetooth       & 6--12 h          \\
        Diadem \cite{Bitbrain_Diadem}                & Bitbrain            & Rigid / Dry          & 12                & 256 Hz                 & --           & Bluetooth             & 8 h              \\
        g.Nautilus \cite{Gtec_GNautilusProFlexible}  & g.tec               & Flexible             & 8 / 16 / 32       & 250 Hz                 & ADS1299      & Proprietary           & 10 h             \\
        Ambulatory platform \cite{pinho2014wireless} & --                  & Active / Dry         & 32                & 250 Hz -- 1 kHz        & ADS1299      & Wi-Fi 802.11 b/g/n    & 26 h             \\
        Neurofeedback system \cite{Totev2023}        & --                  & Passive / Dry        & 40                & 250 Hz                 & ADS1298      & RF                    & --               \\
        BEATS \cite{Beats}                           & --                  & Flexible / Wet       & 32                & 4 kHz                  & ADS1299      & Wi-Fi                 & 24 h (wired)     \\ \bottomrule
    \end{tabularx}
\end{table}

The widespread adoption of these specialized components is evident across both commercial and research-grade platforms, which exhibit a broad spectrum of design trade-offs between channel density, portability, and the temporal precision required for accurate biomarker synchronization. As summarized in Table 1, systems relying on dedicated analog front-ends like the Texas Instruments ADS1299 or ADS1298—such as the OpenBCI Cyton + Daisy, g.Nautilus, the ambulatory platform proposed by Pinho et al., the neurofeedback system by Totev et al., and the BEATS system—leverage these components to guarantee high-performance signal conversion. The Cyton + Daisy system, for instance, supports up to 16 channels and offers a highly versatile sampling rate ranging from 250 Hz to 16 kHz, transferring data via RF, Bluetooth Low Energy, or Wi-Fi to adapt to various environments. Similarly, the actiCAP system achieves high-resolution acquisition up to 16 kHz over a robust USB protocol, making it ideal for long-duration, stable clinical environments. Conversely, more compact, consumer-oriented architectures like the Emotiv EPOC X and the Bitbrain Diadem operate at significantly lower sampling rates of 128 Hz and 256 Hz, respectively. While these devices offer excellent portability and sufficient battery life for general neurofeedback or basic cognitive training, their lower sampling rates and reliance on standard Bluetooth stacks introduce significant vulnerabilities regarding absolute temporal alignment. For clinical research demanding the precise temporal locking of Event-Related Potentials to digital stimuli, architectures that prioritize high-frequency sampling and high-bandwidth or proprietary data transmission—such as the 4 kHz sampling rate of the BEATS system or the custom protocols of the g.Nautilus—are strictly necessary to minimize the non-deterministic communication jitter that otherwise obliterates biological synchronization.

Maximizing these hardware capabilities within densely populated mixed-signal environments requires meticulous printed circuit board design, prominently featuring the strict spatial and galvanic isolation of analog and digital ground planes \cite{wang2024high}. These planes are typically connected only at a single star point to prevent high-frequency digital return currents from modulating the sensitive analog ground reference \cite{sen2025low}. Furthermore, the implementation of dynamic right-leg drive (DRL) circuits—which compute the common-mode average of the measuring electrodes, invert the phase, and feed it back to the subject's body—is essential for stabilizing the baseline potential \cite{luo2025portable}. This active feedback loop drastically improves the overall Common-Mode Rejection Ratio (\gls{CMRR}) of the system, actively neutralizing pervasive power-line interference directly at the source \cite{wen2025analog}.

To accommodate pediatric populations and highly active users where traditional abrasive skin preparation is impossible, modern designs are transitioning from wet silver/silver-chloride electrodes to active dry sensors or polymer-based microneedle arrays \cite{liu2024multichannel, kim2024skin}. By embedding unity-gain operational amplifiers directly at the scalp site, these active shields act as immediate impedance transformers. This active buffering effectively counters the inherently high skin-electrode impedance of dry contacts—often exceeding 100 k$\Omega$ \cite{xiong2025advancements}—and eliminates the triboelectric cable noise and capacitive signal attenuation that would otherwise obliterate the neural signal before it reaches the amplification stage, thus maintaining high signal fidelity in naturalistic environments \cite{giangrande2024motion}.


\subsubsection{Statistical and Decomposition-Based Signal Processing}

Even with optimal hardware-level isolation and active buffering, physiological artifacts originating from the user—such as electrooculograms from eye blinks and electromyograms from jaw clenching or facial muscle movement—inevitably overlap with the low-frequency EEG spectrum \cite{agounad2025advanced, yedukondalu2023circulant}. A mathematically rigorous approach to address this spectral overlap relies on multivariate statistical methods and signal decomposition to computationally unmix these noise sources from the underlying neural activity. While Independent Component Analysis (ICA) remains one of the most widely cited techniques due to its ability to blindly separate statistically independent non-Gaussian sources \cite{avital2025optimizing}, contemporary edge-computing literature highlights its severe operational limitations. ICA requires significant, continuous data buffering to construct a covariance matrix and converge upon a stable demixing matrix, making it highly computationally expensive, memory-intensive, and fundamentally poorly suited for the hard real-time, low-latency requirements of a resource-constrained embedded device \cite{ein2023eeg, shahshahani2022fica}.

To circumvent the massive computational payloads of ICA, modern embedded designs have shifted toward Canonical Correlation Analysis (CCA) and Empirical Mode Decomposition (EMD) \cite{trong2024real}. CCA is highly efficient at isolating and removing high-frequency muscle artifacts by separating sources based on second-order statistics and temporal autocorrelation rather than strict higher-order statistical independence, drastically reducing the requisite processing cycles \cite{hossain2022motion, akshath2025hybrid}.  Concurrently, EMD functions as a purely data-driven heuristic that breaks down non-stationary EEG time series into a finite set of adaptive Intrinsic Mode Functions \cite{gorur2023fourier}. This allows for the robust, automatic template-matching and extraction of low-frequency eye blink artifacts without requiring external reference channels.

Additionally, Discrete Wavelet Transforms are frequently utilized within this philosophical framework to decompose the signal into distinct time-frequency detail and approximation coefficients \cite{zangeneh2022eeg}. This effectively strips out baseline wander via soft thresholding before reconstructing the time-domain signal \cite{serbes2024robust}. However, researchers increasingly caution that the recursive sifting processes inherent to EMD and the multi-level filter banks of wavelet decomposition still pose a distinct risk of central processing exhaustion if not heavily optimized mathematically for the specific instruction sets of the deployment hardware\cite{erbsloh2024technical}.

\subsubsection{Lightweight Deep Learning for Real-Time Denoising}

Driven by the mathematical limitations, rigid linear assumptions, and computational bottlenecks of traditional algorithmic decomposition, the application of Deep Learning for end-to-end, real-time EEG artifact removal has emerged as a disruptive and dominant methodology \cite{azhar2024convolutional}. These neural network architectures bypass the need for manual feature extraction, instead learning to map highly complex, non-linear representations between noisy physiological inputs and clean neural targets through manifold learning \cite{xiong2024general}. However, successfully deploying deep learning on embedded platforms requires severe architectural pruning and quantization to adhere to strict memory footprints and latency constraints \cite{popa2026end}.

The state of the art heavily favors 1D Convolutional Neural Networks because their localized receptive fields natively preserve the temporal and structural integrity of the one-dimensional EEG time series \cite{ige2024state}.  By sliding temporal filters across the raw data stream, these networks utilize a mere fraction of the trainable parameters required by traditional 2D image-based networks \cite{saha2025time}. This crucial architectural choice entirely eliminates the massive computational overhead needed to constantly convert continuous EEG streams into spectrograms via Short-Time Fourier Transforms prior to inference, a process that historically crippled edge devices \cite{nayana2025eeg}.

Furthermore, lightweight Denoising Autoencoders, particularly those constructed with gated recurrent units to capture long-term temporal dependencies, show exceptional promise in isolating patient-specific noise profiles \cite{zhang2022two}. By compressing the noisy multichannel input into a tightly constrained lower-dimensional latent space, the network is forced to discard anomalous artifact variance and learn only the fundamental, high-variance physiological features of genuine brain activity, allowing the decoder to reconstruct a purely neural signal \cite{chuang2022icunet}. These ultra-low-power models consistently achieve significant signal-to-noise ratio improvements while maintaining sub-50 millisecond inference latencies, proving highly viable for real-time edge deployment while deliberately avoiding the quadratic memory complexity and processing overhead associated with massive foundation models \cite{xing2024deep}.

\subsubsection{Edge-Computing Resource Management and Workload Offloading}

The final philosophy addressing embedded EEG constraints focuses entirely on the systemic orchestration of hardware resources and network topologies to prevent processing exhaustion during continuous, high-frequency data ingestion \cite{wang2026noninvasive}. Unoptimized continuous data logging invariably leads to buffer overflows and dropped data packets \cite{kanellopoulos2023networking}. This physical failure fundamentally corrupts the structural continuity of the EEG stream and introduces critical timing errors \cite{kargarnovin2023evidence}. Preventing this jitter is paramount, as the exact synchronization of biomarkers across multiple physiological data streams dictates the temporal validity and overall scientific integrity of the entire BCI system \cite{mullerputz2015towards}.

To mitigate this operational risk, modern Internet of Medical Things paradigms leverage distributed edge computing architectures that strictly segregate data acquisition from intensive computation \cite{brad2024iot}.  In these frameworks, a low-power microcontroller is relegated strictly to the role of a deterministic acquisition gateway. It interfaces with the analog sensors utilizing strict hardware interrupts and direct memory access controllers to autonomously move incoming multi-bit samples directly into alternating ping-pong memory buffers \cite{zayed2025efficient}. Because this process occurs at the hardware level, it does not consume a single central processing clock cycle, ensuring perfect temporal spacing between samples \cite{dobrescu2024direct}.

Once a buffer is filled, this raw, precisely timed data is pushed asynchronously to a secondary, localized computing node responsible for heavy processing \cite{an2025multi}. To prevent this secondary node from thermal throttling during advanced deep learning inference, systems employ compressive offloading techniques, such as immediately extracting frequency-embedded power spectral density features or aggressively downsampling the raw stream to shrink the active memory footprint before classification \cite{akor2025attention}. Most importantly, utilizing sophisticated multi-threading environments ensures the absolute decoupling of the acquisition and processing pipelines; a high-priority hardware thread strictly handles continuous data ingestion, while lower-priority asynchronous threads manage the heavy algorithmic denoising \cite{van2026open}. This architectural isolation guarantees that sudden computational spikes in the processing layer can never stall the hardware layer, thereby guaranteeing the continuous, deterministic sampling rates required for flawless biomarker synchronization \cite{savas2025future}.

\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{forest}
            % Tree Configuration
            for tree={
            grow'=0,               % Grow to the right
            parent anchor=east,    % Edges start from east
            child anchor=west,     % Edges end at west
            anchor=west,           % Align text to the left
            calign=center,         % Center alignment
            edge path={
                    \noexpand\path[\forestoption{edge}]
                    (!u.parent anchor) -- +(10pt,0) |- (.child anchor)\forestoption{edge label};
                },
            font=\sffamily,
            l sep=1cm,             % Horizontal separation
            s sep=0.6cm,           % Vertical separation
            }
            % ----------------------------------------------------------
            % TREE CONTENT
            % ----------------------------------------------------------
            [\textbf{\Large SNR Optimization} \\ \textbf{\Large \& Resource Mgmt.}, align=center
            [{\textbf{Hardware \& AFE} \\ (Isolation)}, align=center
            [\textbullet\ PCB Isolation \& DRL (\cite{li2024assessing, luo2025portable}) \\
            \textbullet\ Dedicated 24-bit ADC (\cite{chen2022124}) \\
            \textbullet\ Active Dry Sensors (\cite{xiong2025advancements}),
            align=left, name=hwlist]
            ]
            [{\textbf{Signal Decomposition} \\ (Statistical)}, align=center
            [\textbullet\ Canonical Correlation (\cite{akshath2025hybrid}) \\
            \textbullet\ Empirical Mode Decomp (\cite{gorur2023fourier}) \\
            \textbullet\ Discrete Wavelet Transforms (\cite{serbes2024robust}),
            align=left, name=statlist]
            ]
            [{\textbf{Lightweight Deep Learning} \\ (Denoising)}, align=center
            [\textbullet\ 1D CNNs (\cite{saha2025time}) \\
            \textbullet\ Denoising Autoencoders (\cite{zhang2022two}) \\
            \textbullet\ GRU Temporal Models (\cite{chuang2022icunet}),
            align=left, name=dllist]
            ]
            [{\textbf{Edge Offloading} \\ (Resource Mgmt)}, align=center
            [\textbullet\ Hardware DMA (\cite{zayed2025efficient}) \\
            \textbullet\ Compressive Offloading (\cite{akor2025attention}) \\
            \textbullet\ Asynchronous Threads (\cite{van2026open}),
            align=left, name=offlist]
            ]
            ]
            % ----------------------------------------------------------
            % ANNOTATIONS (Curly Braces)
            % ----------------------------------------------------------
            \node[description] at (hwlist.east) (hwdesc) {
                (+) High CMRR / low noise \\
                (-) Hardware complexity
            };
            \draw[mybrace] (hwlist.north east) -- (hwlist.south east);
            \node[description] at (statlist.east) (statdesc) {
                (+) Robust artifact removal \\
                (-) Computationally expensive
            };
            \draw[mybrace] (statlist.north east) -- (statlist.south east);
            \node[description] at (dllist.east) (dldesc) {
                (+) Real-time non-linear map \\
                (-) Strict memory limits
            };
            \draw[mybrace] (dllist.north east) -- (dllist.south east);
            \node[description] at (offlist.east) (offdesc) {
                (+) Prevents buffer overflow \\
                (-) Thread sync complexity
            };
            \draw[mybrace] (offlist.north east) -- (offlist.south east);
        \end{forest}%
    }
    \caption{Taxonomy of SNR optimization and resource management strategies in mixed-signal embedded EEG.}
    \label{fig:forest_snr_resource}
\end{figure}

\subsection{Temporal Synchronization and Latency Variability in EEG Biomarkers}

While securing a pristine, high-signal-to-noise ratio analog signal is the strict prerequisite of neurophysiological monitoring, the extraction of valid neurocognitive metrics in therapeutic interventions depends entirely on absolute temporal determinism \cite{norskov2025estimating}. Clinical cognitive assessments evaluate higher-order executive functions—such as sustained attention, target discrimination, and inhibitory control—by analyzing Event-Related Potentials (ERPs). Standard clinical features for these functions include the P300 wave, representing attention allocation and context updating \cite{zygouris2025associations}, and the N200 wave, representing conflict monitoring and impulse inhibition \cite{fazel2024unraveling}. Crucially, these neurophysiological markers are defined not just by their morphology, but by their strict temporal latency relative to a specific external digital stimulus \cite{li2024assessing}.

If the temporal alignment between the digital stimulus event and the biological response in the electroencephalographic (EEG) data stream is skewed by non-deterministic communication jitter or cumulative hardware clock drift \cite{kothe2025lab}, the resulting ERPs will be structurally flattened or completely obliterated during the signal averaging process \cite{molina2024enhanced}. Resolving these timing discrepancies is the absolute foundation for the accurate synchronization of biomarkers across multi-modal data streams \cite{manivannan2025review}. Current research dedicated to bounding this latency and ensuring structural temporal continuity falls into four distinct methodologies \cite{choi2023prefrontal}.

\subsubsection{Hardware-Bounded Event Marking and Interrupt Servicing}

The most rigorous and deterministic approach to synchronization relies on completely bypassing the operating system's software stack to generate and handle physical hardware interrupts \cite{eckhoff2024temporal}. Modern operating systems utilized on edge computing nodes typically employ preemptive multitasking schedulers. When a cognitive event occurs within a digital interface, the transmission of the corresponding marker via standard serial communication stacks is subject to the operating system's internal polling rate, payload encapsulation delays, and unpredictable buffer queuing \cite{miziara2025comparative}. This computational overhead creates a non-deterministic communication jitter that can fluctuate by tens of milliseconds from one trial to the next, entirely corrupting the precision required for high-frequency neural analysis \cite{gemborn2023open}.

To achieve sub-millisecond latency bounding, recent studies heavily advocate for direct hardware triggering and low-level peripheral optimization.  When serial bus communication is mandatory, mitigating latency requires highly optimized endpoint configurations. By forcing communication peripherals to operate in strict interrupt or isochronous transfer modes rather than standard bulk transfers, the acquisition system can guarantee dedicated bus bandwidth and reduce polling intervals to the absolute microframe limits inherent to the communication specification \cite{rousseau2025implementation}. These digital event codes must be parsed at the lowest hardware level via fast interrupt service routines and instantaneously stamped against a highly precise master hardware timer before being merged sequentially into the continuous biological data payload, ensuring zero software-induced jitter \cite{andrijevic2025precision}.

\subsubsection{Protocol-Level Middleware and Network Synchronization}

When purely hardware-based triggering via direct pin manipulation is not feasible—often due to the locked-down nature of commercial consumer hardware used for visual stimuli—the literature relies on advanced software synchronization middleware \cite{lorenz2024review}. Specialized networking protocols have emerged as the ubiquitous standard in modern continuous multi-modal monitoring. These middleware systems are designed explicitly to handle the unified collection of time-series data across disparate, distributed devices over local networks without requiring physical trigger cables \cite{daza2025multimodal}.

These software systems achieve sub-millisecond synchronization accuracy by implementing a continuous background clock-offset measurement. When a digital interface registers a cognitive event, it pushes the marker to a local network outlet, while the sensing unit simultaneously pushes the biological stream to a parallel outlet \cite{klumpp2025syntalos}. The core protocol continuously calculates the network transmission delay and the shifting offset between the distinct local clocks of the distributed devices. It utilizes this data to retroactively adjust the timestamps of the digital markers to perfectly align with the incoming data stream \cite{dasenbrock2022synchronization}. However, researchers caution that these protocols are still bounded by the visual rendering latency of the screen; failing to account for display monitor refresh delays allows mechanical hardware lag to masquerade as delayed human neural processing \cite{han2022assessing}.

\subsubsection{Algorithmic Mitigation of Cumulative Clock Drift}

While single-event latency and jitter corrupt individual evaluation trials, extended continuous monitoring sessions expose the acquisition architecture to cumulative temporal errors known as clock drift. Clock drift occurs because the independent crystal oscillators driving the analog-to-digital converters, the microcontrollers, and the visual display units operate at slightly different, imperfect physical frequencies \cite{ionescu2022synchronization}.  These oscillator frequencies fluctuate further throughout a session due to internal thermal dynamics, battery voltage variations, and ambient environmental conditions. Over an extended therapeutic session, a microscopic drift of just 10 parts-per-million between distinct hardware clocks translates to an absolute temporal misalignment of tens of milliseconds \cite{ding2025wireless}.

This magnitude of drift is severe enough to completely invalidate the precise measurement of fast cognitive potentials, mathematically forcing a healthy neural response to appear delayed or pathological by the end of the session \cite{getzmann2024auditory}. To counter this, modern architectures employ dynamic drift compensation algorithms utilizing linear regression timestamping. By periodically transmitting a specific synchronization pulse from the stimulus generator to the acquisition node at strict intervals, the processing layer can continuously map the digital timeline onto the biological timeline using a rolling linear regression model \cite{li2022intelligent}. This allows the system to calculate the exact drift coefficient in real-time and dynamically resample the data or adjust the event timestamps mathematically, ensuring perfect temporal determinism from the first minute of the session to the last \cite{weber2025methods}.

\subsubsection{Physiological Baseline Validation and Neurocognitive Extraction}

Resolving mechanical, network, and computational timing issues is scientifically insufficient if the system cannot ultimately prove it is capturing authentic, time-locked neurobiology. Consequently, the literature mandates rigorous physiological baseline testing prior to extracting complex cognitive markers \cite{rykov2024predicting}. The most universally accepted validation protocol for continuous temporal bounding is the alpha-band attenuation test, colloquially known as the Berger effect \cite{ha2023validation}.  Alpha waves are highly synchronized, high-amplitude oscillations dominant in states of wakeful relaxation with closed eyes. When a subject opens their eyes to engage with a visual stimulus, the alpha rhythm exhibits an immediate, sharp event-related desynchronization \cite{catalano2024peak}.

By sending a digital marker exactly when the subject is instructed to open their eyes, the system can objectively prove its end-to-end synchronization integrity by tracking the absolute latency between the digital marker and the sudden drop in neural power spectral density \cite{palumbo2024wearable}.  If the system can reliably capture this spontaneous frequency modulation, it validates the entire pipeline for the extraction of highly sensitive, time-locked ERPs associated with targeting stimuli or withholding a response \cite{wascher2023neuroergonomics}. The ability to perfectly fuse the digital interaction state with these precise analog signals confirms that the hardware isolation, noise reduction algorithms, and synchronization protocols have collectively succeeded in achieving the exact synchronization of biomarkers required for clinical analysis \cite{lin2023design}.

\begin{figure}[H]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{forest}
            % Tree Configuration
            for tree={
            grow'=0,               % Grow to the right
            parent anchor=east,    % Edges start from east
            child anchor=west,     % Edges end at west
            anchor=west,           % Align text to the left
            calign=center,         % Center alignment
            edge path={
                    \noexpand\path[\forestoption{edge}]
                    (!u.parent anchor) -- +(10pt,0) |- (.child anchor)\forestoption{edge label};
                },
            font=\sffamily,
            l sep=1cm,             % Horizontal separation
            s sep=0.6cm,           % Vertical separation
            }
            % ----------------------------------------------------------
            % TREE CONTENT
            % ----------------------------------------------------------
            [\textbf{\Large Temporal Synchronization} \\ \textbf{\Large \& Latency}, align=center
            [{\textbf{Hardware Triggering} \\ (Event Marking)}, align=center
            [\textbullet\ Isochronous Modes (\cite{rousseau2025implementation}) \\
            \textbullet\ Master Timers (\cite{andrijevic2025precision}) \\
            \textbullet\ Pin Manipulation (\cite{eckhoff2024temporal}),
            align=left, name=hwtriglist]
            ]
            [{\textbf{Protocol Middleware} \\ (Network Sync)}, align=center
            [\textbullet\ Net Transmission Delay (\cite{klumpp2025syntalos}) \\
            \textbullet\ Clock-Offset Measurement (\cite{dasenbrock2022synchronization}) \\
            \textbullet\ Distributed Unity (\cite{daza2025multimodal}),
            align=left, name=netlist]
            ]
            [{\textbf{Drift Mitigation} \\ (Algorithmic)}, align=center
            [\textbullet\ Linear Regression Model (\cite{li2022intelligent}) \\
            \textbullet\ Dynamic Resampling (\cite{weber2025methods}) \\
            \textbullet\ Rolling Drift Comp (\cite{ionescu2022synchronization}),
            align=left, name=driftlist]
            ]
            [{\textbf{Baseline Validation} \\ (Physiological)}, align=center
            [\textbullet\ Alpha Attenuation (\cite{ha2023validation}) \\
            \textbullet\ Peak Latency Tracking (\cite{catalano2024peak}) \\
            \textbullet\ ERP Temporal Fusion (\cite{lin2023design}),
            align=left, name=baselist]
            ]
            ]
            % ----------------------------------------------------------
            % ANNOTATIONS (Curly Braces)
            % ----------------------------------------------------------
            \node[description] at (hwtriglist.east) (hwtrigdesc) {
                (+) Sub-ms precision \\
                (-) Highly specific
            };
            \draw[mybrace] (hwtriglist.north east) -- (hwtriglist.south east);
            \node[description] at (netlist.east) (netdesc) {
                (+) Highly scalable \\
                (-) Bounded by OS networking
            };
            \draw[mybrace] (netlist.north east) -- (netlist.south east);
            \node[description] at (driftlist.east) (driftdesc) {
                (+) Long-term valid \\
                (-) Needs sync pulses
            };
            \draw[mybrace] (driftlist.north east) -- (driftlist.south east);
            \node[description] at (baselist.east) (basedesc) {
                (+) End-to-end proof \\
                (-) Depends on compliance
            };
            \draw[mybrace] (baselist.north east) -- (baselist.south east);
        \end{forest}%
    }
    \caption{Taxonomy of methodologies for temporal synchronization and latency bounding in EEG biomarkers.}
    \label{fig:forest_temporal_sync}
\end{figure}

In summary, while current literature provides comprehensive methodologies for EEG acquisition, adapting these for clinical-grade pediatric \gls{ADHD} assessment requires navigating the specific limitations of existing embedded architectures. For the first defined challenge overcoming \gls{SNR} limitations and resource exhaustion dedicated high-resolution analog front-ends combined with active grounding and distributed edge-offloading present the most viable architecture. However, many current statistical and deep learning-based denoising alternatives impose computational overheads that inherently threaten real-time stability on constrained systems. Consequently, outcomes for this physical and computational problem must be explicitly measured by quantifying baseline input-referred noise against commercial standards and by conducting stress tests that monitor continuous CPU and RAM utilization to preclude data packet loss during high-frequency streaming. For the second challenge temporal synchronization and latency variability relying solely on software-level middleware or network protocols is limited by unpredictable operating system scheduling and display refresh delays. The most promising alternative rests on hardware-bounded event marking synchronized via explicit interrupt service routines. To validate outcomes for this synchronization challenge, evaluations must rigorously quantify short-term, non-deterministic communication jitter and measure cumulative clock drift over extended, full-length therapeutic sessions. Finally, these temporal metrics must be biologically anchored by executing physiological baseline validations, such as the alpha-band attenuation test, alongside strict signal isolation assessments to verify that the extracted, time-locked biomarkers are free from spatial crosstalk and genuinely reflect neurocognitive engagement.



