@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{qiang2022attcat,
  title={Attcat: Explaining transformers via attentive class activation tokens},
  author={Qiang, Yao and Pan, Deng and Li, Chengyin and Li, Xin and Jang, Rhongho and Zhu, Dongxiao},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={5052--5064},
  year={2022}
}

@article{abnar2020quantifying,
  title={Quantifying attention flow in transformers},
  author={Abnar, Samira and Zuidema, Willem},
  journal={arXiv preprint arXiv:2005.00928},
  year={2020}
}

@inproceedings{tanaka2021visualmrc,
  title={Visualmrc: Machine reading comprehension on document images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Yoshida, Sen},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={15},
  pages={13878--13888},
  year={2021}
}


@article{torfi2020natural,
  title={Natural language processing advancements by deep learning: A survey},
  author={Torfi, Amirsina and Shirvani, Rouzbeh A and Keneshloo, Yaser and Tavaf, Nader and Fox, Edward A},
  journal={arXiv preprint arXiv:2003.01200},
  year={2020}
}


@misc{natural_language_processing_market_growth_report_2030_2030, title={Natural Language Processing Market Growth Report, 2030}, url={https://www.grandviewresearch.com/industry-analysis/natural-language-processing-market-report}, year={2030}, month={Apr} 
}


@INPROCEEDINGS{7583963,
  author={Lende, Sweta P. and Raghuwanshi, M. M.},
  booktitle={2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare (Startup Conclave)}, 
  title={Question answering system on education acts using NLP techniques}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/STARTUP.2016.7583963}}

@article{CAO2010962,
title = {Automatically extracting information needs from complex clinical questions},
journal = {Journal of Biomedical Informatics},
volume = {43},
number = {6},
pages = {962-971},
year = {2010},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2010.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1532046410001061},
author = {Yong-gang Cao and James J. Cimino and John Ely and Hong Yu},
keywords = {Natural language processing, Question answering, Question analysis, Keyword extraction},
abstract = {Objective
Clinicians pose complex clinical questions when seeing patients, and identifying the answers to those questions in a timely manner helps improve the quality of patient care. We report here on two natural language processing models, namely, automatic topic assignment and keyword identification, that together automatically and effectively extract information needs from ad hoc clinical questions. Our study is motivated in the context of developing the larger clinical question answering system AskHERMES (Help clinicians to Extract and aRrticulate Multimedia information for answering clinical quEstionS).
Design and measurements
We developed supervised machine-learning systems to automatically assign predefined general categories (e.g. etiology, procedure, and diagnosis) to a question. We also explored both supervised and unsupervised systems to automatically identify keywords that capture the main content of the question.
Results
We evaluated our systems on 4654 annotated clinical questions that were collected in practice. We achieved an F1 score of 76.0% for the task of general topic classification and 58.0% for keyword extraction. Our systems have been implemented into the larger question answering system AskHERMES. Our error analyses suggested that inconsistent annotation in our training data have hurt both question analysis tasks.
Conclusion
Our systems, available at http://www.askhermes.org, can automatically extract information needs from both short (the number of word tokens <20) and long questions (the number of word tokens >20), and from both well-structured and ill-formed questions. We speculate that the performance of general topic classification and keyword extraction can be further improved if consistently annotated data are made available.}
}

@INPROCEEDINGS{7755228,
  author={Pudaruth, Sameerchand and Boodhoo, Kajal and Goolbudun, Lushika},
  booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, 
  title={An intelligent question answering system for ICT}, 
  year={2016},
  volume={},
  number={},
  pages={2895-2899},
  doi={10.1109/ICEEOT.2016.7755228}}


@ARTICLE{6177724,
  author={Ferrucci, D. A.},
  journal={IBM Journal of Research and Development}, 
  title={Introduction to “This is Watson”}, 
  year={2012},
  volume={56},
  number={3.4},
  pages={1:1-1:15},
  doi={10.1147/JRD.2012.2184356}}

@ARTICLE{Peng2022,
  author = {Zhong, Shan and Peng, Jie and Xu, Jianhui},
  title = {Personalized Product Recommendation Model of Automatic Question Answering Robot Based on Deep Learning},
  journal = {Journal of Robotics},
  volume = {2022},
  pages = {1256083},
  year = {2022},
  date = {2022-03-22},
  doi = {10.1155/2022/1256083},
  publisher = {Hindawi},
  url = {https://doi.org/10.1155/2022/1256083},
}

@ARTICLE{9072442,
  author={Huang, Zhen and Xu, Shiyi and Hu, Minghao and Wang, Xinyi and Qiu, Jinyan and Fu, Yongquan and Zhao, Yuncai and Peng, Yuxing and Wang, Changjian},
  journal={IEEE Access}, 
  title={Recent Trends in Deep Learning Based Open-Domain Textual Question Answering Systems}, 
  year={2020},
  volume={8},
  number={},
  pages={94341-94356},
  doi={10.1109/ACCESS.2020.2988903}}


@article{shah2020chatbot,
  title={Chatbot analytics based on question answering system and deep learning: case study for movie smart automatic answering},
  author={Shah, Jugal and Mohammed, Sabah},
  journal={Int. J. Softw. Eng. Its Appl},
  volume={14},
  pages={7--16},
  year={2020}
}

@INPROCEEDINGS{9544654,
  author={Kumar, Alok and Kharadi, Aditi and Singh, Deepika and Kumari, Mala},
  booktitle={2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA)}, 
  title={Automatic question-answer pair generation using Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={794-799},
  doi={10.1109/ICIRCA51532.2021.9544654}}

@article{hadi2023large,
  title={Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects},
  author={Hadi, Muhammad Usman and Qureshi, Rizwan and Shah, Abbas and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali and others},
  journal={Authorea Preprints},
  year={2023},
  publisher={Authorea}
}

@article{pearce2021comparative,
  title={A comparative study of transformer-based language models on extractive question answering},
  author={Pearce, Kate and Zhan, Tiffany and Komanduri, Aneesh and Zhan, Justin},
  journal={arXiv preprint arXiv:2110.03142},
  year={2021}
}


@article{thayaparan2020explanationlp,
  title={Explanationlp: Abductive reasoning for explainable science question answering},
  author={Thayaparan, Mokanarangan and Valentino, Marco and Freitas, Andr{\'e}},
  journal={arXiv preprint arXiv:2010.13128},
  year={2020}
}

@article{dao2020demystifying,
  title={Demystifying deep neural networks through interpretation: A survey},
  author={Dao, Giang and Lee, Minwoo},
  journal={arXiv preprint arXiv:2012.07119},
  year={2020}
}

@article{caldwell2022agile,
  title={An agile new research framework for hybrid human-AI teaming: Trust, transparency, and transferability},
  author={Caldwell, Sabrina and Sweetser, Penny and O’Donnell, Nicholas and Knight, Matthew J and Aitchison, Matthew and Gedeon, Tom and Johnson, Daniel and Brereton, Margot and Gallagher, Marcus and Conroy, David},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={12},
  number={3},
  pages={1--36},
  year={2022},
  publisher={ACM New York, NY}
}

@article{yusuf2022analysis,
  title={An analysis of graph convolutional networks and recent datasets for visual question answering},
  author={Yusuf, Abdulganiyu Abdu and Chong, Feng and Xianling, Mao},
  journal={Artificial Intelligence Review},
  volume={55},
  number={8},
  pages={6277--6300},
  year={2022},
  publisher={Springer}
}

@article{gao2023improve,
  title={How to improve the application potential of deep learning model in HVAC fault diagnosis: Based on pruning and interpretable deep learning method},
  author={Gao, Yuan and Miyata, Shohei and Akashi, Yasunori},
  journal={Applied Energy},
  volume={348},
  pages={121591},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{NEURIPS2022_20e45668,
 author = {Qiang, Yao and Pan, Deng and Li, Chengyin and Li, Xin and Jang, Rhongho and Zhu, Dongxiao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {5052--5064},
 publisher = {Curran Associates, Inc.},
 title = {AttCAT: Explaining Transformers via Attentive Class Activation Tokens},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/20e45668fefa793bd9f2edf19be12c4b-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{visualmrc,
  author       = {Ryota Tanaka and
                  Kyosuke Nishida and
                  Sen Yoshida},
  title        = {VisualMRC: Machine Reading Comprehension on Document Images},
  journal      = {CoRR},
  volume       = {abs/2101.11272},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.11272},
  eprinttype    = {arXiv},
  eprint       = {2101.11272},
  timestamp    = {Sun, 31 Jan 2021 17:23:50 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2101-11272.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@MISC{goodfellow2016dive,
  author = {Goodfellow, I., Bengio, Y., & Courville, A.},
  booktitle={Dive to Deep Learning},
  url={https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html},
  year = {2016},
  title = {Dive to Deep Learning},
  source = {Dive into Deep Learning},
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{7583963,
  author={Lende, Sweta P. and Raghuwanshi, M. M.},
  booktitle={2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare (Startup Conclave)}, 
  title={Question answering system on education acts using NLP techniques}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/STARTUP.2016.7583963}}

@article{CAO2010962,
title = {Automatically extracting information needs from complex clinical questions},
journal = {Journal of Biomedical Informatics},
volume = {43},
number = {6},
pages = {962-971},
year = {2010},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2010.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1532046410001061},
author = {Yong-gang Cao and James J. Cimino and John Ely and Hong Yu},
keywords = {Natural language processing, Question answering, Question analysis, Keyword extraction},
abstract = {Objective
Clinicians pose complex clinical questions when seeing patients, and identifying the answers to those questions in a timely manner helps improve the quality of patient care. We report here on two natural language processing models, namely, automatic topic assignment and keyword identification, that together automatically and effectively extract information needs from ad hoc clinical questions. Our study is motivated in the context of developing the larger clinical question answering system AskHERMES (Help clinicians to Extract and aRrticulate Multimedia information for answering clinical quEstionS).
Design and measurements
We developed supervised machine-learning systems to automatically assign predefined general categories (e.g. etiology, procedure, and diagnosis) to a question. We also explored both supervised and unsupervised systems to automatically identify keywords that capture the main content of the question.
Results
We evaluated our systems on 4654 annotated clinical questions that were collected in practice. We achieved an F1 score of 76.0% for the task of general topic classification and 58.0% for keyword extraction. Our systems have been implemented into the larger question answering system AskHERMES. Our error analyses suggested that inconsistent annotation in our training data have hurt both question analysis tasks.
Conclusion
Our systems, available at http://www.askhermes.org, can automatically extract information needs from both short (the number of word tokens <20) and long questions (the number of word tokens >20), and from both well-structured and ill-formed questions. We speculate that the performance of general topic classification and keyword extraction can be further improved if consistently annotated data are made available.}
}

@INPROCEEDINGS{7755228,
  author={Pudaruth, Sameerchand and Boodhoo, Kajal and Goolbudun, Lushika},
  booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, 
  title={An intelligent question answering system for ICT}, 
  year={2016},
  volume={},
  number={},
  pages={2895-2899},
  doi={10.1109/ICEEOT.2016.7755228}}

@ARTICLE{6177724,
  author={Ferrucci, D. A.},
  journal={IBM Journal of Research and Development}, 
  title={Introduction to “This is Watson”}, 
  year={2012},
  volume={56},
  number={3.4},
  pages={1:1-1:15},
  doi={10.1147/JRD.2012.2184356}}

@ARTICLE{Peng2022,
  author = {Zhong, Shan and Peng, Jie and Xu, Jianhui},
  title = {Personalized Product Recommendation Model of Automatic Question Answering Robot Based on Deep Learning},
  journal = {Journal of Robotics},
  volume = {2022},
  pages = {1256083},
  year = {2022},
  date = {2022-03-22},
  doi = {10.1155/2022/1256083},
  publisher = {Hindawi},
  url = {https://doi.org/10.1155/2022/1256083},
}

@ARTICLE{9072442,
  author={Huang, Zhen and Xu, Shiyi and Hu, Minghao and Wang, Xinyi and Qiu, Jinyan and Fu, Yongquan and Zhao, Yuncai and Peng, Yuxing and Wang, Changjian},
  journal={IEEE Access}, 
  title={Recent Trends in Deep Learning Based Open-Domain Textual Question Answering Systems}, 
  year={2020},
  volume={8},
  number={},
  pages={94341-94356},
  doi={10.1109/ACCESS.2020.2988903}}

@article{shah2020chatbot,
  title={Chatbot analytics based on question answering system and deep learning: case study for movie smart automatic answering},
  author={Shah, Jugal and Mohammed, Sabah},
  journal={Int. J. Softw. Eng. Its Appl},
  volume={14},
  pages={7--16},
  year={2020}
}

@INPROCEEDINGS{9544654,
  author={Kumar, Alok and Kharadi, Aditi and Singh, Deepika and Kumari, Mala},
  booktitle={2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA)}, 
  title={Automatic question-answer pair generation using Deep Learning}, 
  year={2021},
  volume={},
  number={},
  pages={794-799},
  doi={10.1109/ICIRCA51532.2021.9544654}}

@article{hadi2023large,
  title={Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects},
  author={Hadi, Muhammad Usman and Qureshi, Rizwan and Shah, Abbas and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali and others},
  journal={Authorea Preprints},
  year={2023},
  publisher={Authorea}
}

@article{arbaaeen2021ontology,
  title={Ontology-based approach to semantically enhanced question answering for closed domain: A review},
  author={Arbaaeen, Ammar and Shah, Asadullah},
  journal={Information},
  volume={12},
  number={5},
  pages={200},
  year={2021},
  publisher={MDPI}
}

@article{thayaparan2020explanationlp,
  title={Explanationlp: Abductive reasoning for explainable science question answering},
  author={Thayaparan, Mokanarangan and Valentino, Marco and Freitas, Andr{\'e}},
  journal={arXiv preprint arXiv:2010.13128},
  year={2020}
}

@article{dao2020demystifying,
  title={Demystifying deep neural networks through interpretation: A survey},
  author={Dao, Giang and Lee, Minwoo},
  journal={arXiv preprint arXiv:2012.07119},
  year={2020}
}

@article{caldwell2022agile,
  title={An agile new research framework for hybrid human-AI teaming: Trust, transparency, and transferability},
  author={Caldwell, Sabrina and Sweetser, Penny and O’Donnell, Nicholas and Knight, Matthew J and Aitchison, Matthew and Gedeon, Tom and Johnson, Daniel and Brereton, Margot and Gallagher, Marcus and Conroy, David},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={12},
  number={3},
  pages={1--36},
  year={2022},
  publisher={ACM New York, NY}
}

@article{yusuf2022analysis,
  title={An analysis of graph convolutional networks and recent datasets for visual question answering},
  author={Yusuf, Abdulganiyu Abdu and Chong, Feng and Xianling, Mao},
  journal={Artificial Intelligence Review},
  volume={55},
  number={8},
  pages={6277--6300},
  year={2022},
  publisher={Springer}
}

@article{gao2023improve,
  title={How to improve the application potential of deep learning model in HVAC fault diagnosis: Based on pruning and interpretable deep learning method},
  author={Gao, Yuan and Miyata, Shohei and Akashi, Yasunori},
  journal={Applied Energy},
  volume={348},
  pages={121591},
  year={2023},
  publisher={Elsevier}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}

@article{kovaleva2019revealing,
  title={Revealing the dark secrets of BERT},
  author={Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:1908.08593},
  year={2019}
}

@article{serrano2019attention,
  title={Is attention interpretable?},
  author={Serrano, Sofia and Smith, Noah A},
  journal={arXiv preprint arXiv:1906.03731},
  year={2019}
}

@article{jain2019attention,
  title={Attention is not explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  journal={arXiv preprint arXiv:1902.10186},
  year={2019}
}

@article{abnar2020quantifying,
  title={Quantifying attention flow in transformers},
  author={Abnar, Samira and Zuidema, Willem},
  journal={arXiv preprint arXiv:2005.00928},
  year={2020}
}

@inproceedings{chefer2021transformer,
  title={Transformer interpretability beyond attention visualization},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={782--791},
  year={2021}
}

@article{bastings2020elephant,
  title={The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?},
  author={Bastings, Jasmijn and Filippova, Katja},
  journal={arXiv preprint arXiv:2010.05607},
  year={2020}
}

@article{li2022saliency,
  title={Saliency guided adversarial training for learning generalizable features with applications to medical imaging classification system},
  author={Li, Xin and Qiang, Yao and Li, Chengyin and Liu, Sijia and Zhu, Dongxiao},
  journal={arXiv preprint arXiv:2209.04326},
  year={2022}
}

@inproceedings{hao2021self,
  title={Self-attention attribution: Interpreting information interactions inside transformer},
  author={Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={14},
  pages={12963--12971},
  year={2021}
}

@inproceedings{barkan2021grad,
  title={Grad-sam: Explaining transformers via gradient self-attention maps},
  author={Barkan, Oren and Hauon, Edan and Caciularu, Avi and Katz, Ori and Malkiel, Itzik and Armstrong, Omri and Koenigstein, Noam},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={2882--2887},
  year={2021}
}

@article{chrysostomou2021enjoy,
  title={Enjoy the salience: Towards better transformer-based faithful explanations with word salience},
  author={Chrysostomou, George and Aletras, Nikolaos},
  journal={arXiv preprint arXiv:2108.13759},
  year={2021}
}

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}

@inproceedings{chefer2021transformer,
  title={Transformer interpretability beyond attention visualization},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={782--791},
  year={2021}
}

@article{bach2015pixel,
  title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={PloS one},
  volume={10},
  number={7},
  pages={e0130140},
  year={2015},
  publisher={Public Library of Science}
}

@article{montavon2017explaining,
  title={Explaining nonlinear classification decisions with deep taylor decomposition},
  author={Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal={Pattern recognition},
  volume={65},
  pages={211--222},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{chefer2021generic,
  title={Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers},
  author={Chefer, Hila and Gur, Shir and Wolf, Lior},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={397--406},
  year={2021}


@article{pan2020explainable,
  title={Explainable recommendation via interpretable feature mapping and evaluation of explainability},
  author={Pan, Deng and Li, Xiangrui and Li, Xin and Zhu, Dongxiao},
  journal={arXiv preprint arXiv:2007.06133},
  year={2020}
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv 2019},
  author={Sanh, Victor and Debut, L and Chaumond, J and Wolf, T},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}